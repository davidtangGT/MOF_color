{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with hyperas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I originally ran some simple grid search, but the issue there is that I didn't have a carefull split into training and validation set as it was only to see what performance we could get and which way we should go. \n",
    "\n",
    "Apparently---and a bit suprisingly for me---RGB gives us the best performance and so I'll stick with it here. Also we will use the dropout approach for the Bayesian approximation. \n",
    "\n",
    "Also we saw that MinMaxScaler is likly best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dropout, Dense, BatchNormalization, GaussianDropout, GaussianNoise, LeakyReLU\n",
    "lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "#from livelossplot.keras import PlotLossesCallback\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from colorml.utils import mapping_to_target_range, read_pickle, get_timestamp_string, plot_predictions, huber_fn, mapping_to_target_range_sig, augment_data\n",
    "from colorml.descriptornames import * \n",
    "from livelossplot.keras import PlotLossesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#metalcenter_descriptors + functionalgroup_descriptors + linker_descriptors + mol_desc  + summed_linker_descriptors + summed_metalcenter_descriptors + summed_functionalgroup_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    CHEMICAL_FEATURES = metalcenter_descriptors + functionalgroup_descriptors + linker_descriptors + mol_desc\n",
    "    df_subset_merged = pd.read_csv('../data/color_feat_merged.csv')\n",
    "    augment_dict = read_pickle('../data/augment_dict.pkl')\n",
    "    df_train, df_test = train_test_split(df_subset_merged, train_size=0.7)\n",
    "    df_train = augment_data(df_train, augment_dict)\n",
    "\n",
    "    X_train = df_train[CHEMICAL_FEATURES]\n",
    "    y_train = df_train[['r', 'g', 'b']]\n",
    "\n",
    "    X_test = df_test[CHEMICAL_FEATURES]\n",
    "    y_test = df_test[['r', 'g', 'b']]\n",
    "\n",
    "    name_train = df_train['color_cleaned']\n",
    "    name_test = df_test['color_cleaned']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_test, X_valid, y_test, y_valid, names_valid, names_valid = train_test_split(X_test, y_test, name_test, train_size=0.9)\n",
    "\n",
    "    y_train = y_train / 255\n",
    "    y_valid = y_valid / 255\n",
    "    y_test = y_test / 255\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test):\n",
    "    mlp = Sequential()\n",
    "    dropout = {{uniform(0, 1)}}\n",
    "    l1rate = {{uniform(0, 1)}}\n",
    "    layers = {{choice([\n",
    "        \n",
    "        [64, 32, 16, 8], \n",
    "        [128, 64, 32, 16, 8], \n",
    "        [128, 16, 8], \n",
    "        [32, 16, 8],\n",
    "        [16, 8, 8], \n",
    "        [8,8,8,8], \n",
    "        [164,64,32,16],\n",
    "        [164,32,16],\n",
    "        [164,64,8]]\n",
    "    )}}\n",
    "    \n",
    "    mlp.add(\n",
    "        Dense(\n",
    "            layers[0],\n",
    "            activation=\"linear\",\n",
    "            kernel_initializer='he_normal',\n",
    "            input_shape=(178,),\n",
    "            activity_regularizer=l1(l1rate),\n",
    "        )\n",
    "    )\n",
    "    mlp.add(Activation(\"relu\"))\n",
    "    mlp.add(Dropout(dropout))\n",
    "\n",
    "    for layer in layers[1:]:\n",
    "        mlp.add(\n",
    "            Dense(\n",
    "                layer,\n",
    "                activation=\"linear\",\n",
    "                kernel_initializer='he_normal',\n",
    "                activity_regularizer=l1(l1rate),\n",
    "            )\n",
    "        )\n",
    "        mlp.add(Activation(\"relu\"))\n",
    "        mlp.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    mlp.add(\n",
    "        Dense(3, activation=mapping_to_target_range, kernel_initializer='he_normal')\n",
    "    )\n",
    "                    \n",
    "    mlp.compile(\n",
    "        optimizer=Adam(learning_rate={{uniform(1e-5, 1e-2)}}),\n",
    "        loss={{choice([huber_fn, 'mae', 'mse'])}},\n",
    "        metrics=[\"mae\", \"mean_absolute_percentage_error\"],\n",
    "    )\n",
    "    \n",
    "    callbacks = []\n",
    "                    \n",
    "    \n",
    "    if {{choice(['es', False])}} == 'es':\n",
    "        callbacks.append(\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience={{choice([10, 20, 30, 40, 60, 80, 100])}}, verbose=0, mode=\"auto\"\n",
    "                )\n",
    "            )\n",
    "                    \n",
    "    if {{choice(['lrs', False])}} == 'lrs':\n",
    "        callbacks.append(\n",
    "            learning_rate_reduction=ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                patience={{choice([10, 20, 30, 40, 60, 80, 100])}},\n",
    "                verbose=1,\n",
    "                factor={{uniform(0, 1)}},\n",
    "                min_lr={{uniform(1e-5, 1e-2)}},\n",
    "            )\n",
    "        )\n",
    "                    \n",
    "    history = mlp.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            callbacks=callbacks,\n",
    "            epochs=500,\n",
    "            batch_size={{choice([64, 128, 256])}},\n",
    "            validation_data=(X_valid, y_valid),\n",
    "        )\n",
    "                    \n",
    "    score, mae, mape = mlp.evaluate(X_valid, y_valid, verbose=0)\n",
    "                    \n",
    "    return {'loss': -mae, 'status': STATUS_OK, 'model': model}               \n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, Dropout, concatenate, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop, Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l1\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout, Dense, BatchNormalization, GaussianDropout, GaussianNoise, LeakyReLU\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.constraints import MinMaxNorm\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.initializers import Constant\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from colorml.utils import mapping_to_target_range, read_pickle, get_timestamp_string, plot_predictions, huber_fn, mapping_to_target_range_sig, augment_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from colorml.descriptornames import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from livelossplot.keras import PlotLossesCallback\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout': hp.uniform('dropout', 0, 1),\n",
      "        'dropout_1': hp.uniform('dropout_1', 0, 1),\n",
      "        'layers': hp.choice('layers', [\n",
      "        \n",
      "        [64, 32, 16, 8], \n",
      "        [128, 64, 32, 16, 8], \n",
      "        [128, 16, 8], \n",
      "        [32, 16, 8],\n",
      "        [16, 8, 8], \n",
      "        [8,8,8,8], \n",
      "        [164,64,32,16],\n",
      "        [164,32,16],\n",
      "        [164,64,8]]\n",
      "    ),\n",
      "        'learning_rate': hp.uniform('learning_rate', 1e-5, 1e-2),\n",
      "        'loss': hp.choice('loss', [huber_fn, 'mae', 'mse']),\n",
      "        'loss_1': hp.choice('loss_1', ['es', False]),\n",
      "        'patience': hp.choice('patience', [10, 20, 30, 40, 60, 80, 100]),\n",
      "        'patience_1': hp.choice('patience_1', ['lrs', False]),\n",
      "        'patience_2': hp.choice('patience_2', [10, 20, 30, 40, 60, 80, 100]),\n",
      "        'dropout_2': hp.uniform('dropout_2', 0, 1),\n",
      "        'learning_rate_1': hp.uniform('learning_rate_1', 1e-5, 1e-2),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128, 256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: CHEMICAL_FEATURES = metalcenter_descriptors + functionalgroup_descriptors + linker_descriptors + mol_desc\n",
      "  3: df_subset_merged = pd.read_csv('../data/color_feat_merged.csv')\n",
      "  4: augment_dict = read_pickle('../data/augment_dict.pkl')\n",
      "  5: df_train, df_test = train_test_split(df_subset_merged, train_size=0.7)\n",
      "  6: df_train = augment_data(df_train, augment_dict)\n",
      "  7: \n",
      "  8: X_train = df_train[CHEMICAL_FEATURES]\n",
      "  9: y_train = df_train[['r', 'g', 'b']]\n",
      " 10: \n",
      " 11: X_test = df_test[CHEMICAL_FEATURES]\n",
      " 12: y_test = df_test[['r', 'g', 'b']]\n",
      " 13: \n",
      " 14: name_train = df_train['color_cleaned']\n",
      " 15: name_test = df_test['color_cleaned']\n",
      " 16: \n",
      " 17: scaler = MinMaxScaler()\n",
      " 18: X_train = scaler.fit_transform(X_train)\n",
      " 19: X_test = scaler.transform(X_test)\n",
      " 20: \n",
      " 21: X_test, X_valid, y_test, y_valid, names_valid, names_valid = train_test_split(X_test, y_test, name_test, train_size=0.9)\n",
      " 22: \n",
      " 23: y_train = y_train / 255\n",
      " 24: y_valid = y_valid / 255\n",
      " 25: y_test = y_test / 255\n",
      " 26: \n",
      " 27: \n",
      " 28: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     mlp = Sequential()\n",
      "   4:     dropout = space['dropout']\n",
      "   5:     l1rate = space['dropout_1']\n",
      "   6:     layers = space['layers']\n",
      "   7:     \n",
      "   8:     mlp.add(\n",
      "   9:         Dense(\n",
      "  10:             layers[0],\n",
      "  11:             activation=\"linear\",\n",
      "  12:             kernel_initializer='he_normal',\n",
      "  13:             input_shape=(178,),\n",
      "  14:             activity_regularizer=l1(l1rate),\n",
      "  15:         )\n",
      "  16:     )\n",
      "  17:     mlp.add(Activation(\"relu\"))\n",
      "  18:     mlp.add(Dropout(dropout))\n",
      "  19: \n",
      "  20:     for layer in layers[1:]:\n",
      "  21:         mlp.add(\n",
      "  22:             Dense(\n",
      "  23:                 layer,\n",
      "  24:                 activation=\"linear\",\n",
      "  25:                 kernel_initializer='he_normal',\n",
      "  26:                 activity_regularizer=l1(l1rate),\n",
      "  27:             )\n",
      "  28:         )\n",
      "  29:         mlp.add(Activation(\"relu\"))\n",
      "  30:         mlp.add(Dropout(dropout))\n",
      "  31: \n",
      "  32: \n",
      "  33:     mlp.add(\n",
      "  34:         Dense(3, activation=mapping_to_target_range, kernel_initializer='he_normal')\n",
      "  35:     )\n",
      "  36:                     \n",
      "  37:     mlp.compile(\n",
      "  38:         optimizer=Adam(learning_rate=space['learning_rate']),\n",
      "  39:         loss=space['loss'],\n",
      "  40:         metrics=[\"mae\", \"mean_absolute_percentage_error\"],\n",
      "  41:     )\n",
      "  42:     \n",
      "  43:     callbacks = []\n",
      "  44:                     \n",
      "  45:     \n",
      "  46:     if space['loss_1'] == 'es':\n",
      "  47:         callbacks.append(\n",
      "  48:                 EarlyStopping(\n",
      "  49:                     monitor=\"val_loss\", patience=space['patience'], verbose=0, mode=\"auto\"\n",
      "  50:                 )\n",
      "  51:             )\n",
      "  52:                     \n",
      "  53:     if space['patience_1'] == 'lrs':\n",
      "  54:         callbacks.append(\n",
      "  55:             learning_rate_reduction=ReduceLROnPlateau(\n",
      "  56:                 monitor=\"val_loss\",\n",
      "  57:                 patience=space['patience_2'],\n",
      "  58:                 verbose=1,\n",
      "  59:                 factor=space['dropout_2'],\n",
      "  60:                 min_lr=space['learning_rate_1'],\n",
      "  61:             )\n",
      "  62:         )\n",
      "  63:                     \n",
      "  64:     history = mlp.fit(\n",
      "  65:             X_train,\n",
      "  66:             y_train,\n",
      "  67:             callbacks=callbacks,\n",
      "  68:             epochs=500,\n",
      "  69:             batch_size=space['batch_size'],\n",
      "  70:             validation_data=(X_valid, y_valid),\n",
      "  71:         )\n",
      "  72:                     \n",
      "  73:     score, mae, mape = mlp.evaluate(X_valid, y_valid, verbose=0)\n",
      "  74:                     \n",
      "  75:     return {'loss': -mae, 'status': STATUS_OK, 'model': model}               \n",
      "  76: \n",
      "Train on 115276 samples, validate on 192 samples     \n",
      "Epoch 1/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 3:39 - loss: 31503.6094 - mae: 5.1065 - mean_absolute_percentage_error: 109081760.0000\n",
      "                                                    \n",
      "  2816/115276 [..............................]       \n",
      " - ETA: 21s - loss: 10851.5368 - mae: 1.2303 - mean_absolute_percentage_error: 33689788.0000  \n",
      "                                                    \n",
      "  5120/115276 [>.............................]       \n",
      " - ETA: 12s - loss: 8790.7987 - mae: 0.8635 - mean_absolute_percentage_error: 26460826.0000 \n",
      "                                                     \n",
      "  6912/115276 [>.............................]       \n",
      " - ETA: 10s - loss: 7618.9827 - mae: 0.7437 - mean_absolute_percentage_error: 24486542.0000\n",
      "                                                     \n",
      "  9216/115276 [=>............................]       \n",
      " - ETA: 8s - loss: 6450.5180 - mae: 0.6538 - mean_absolute_percentage_error: 23911164.0000 \n",
      "                                                    \n",
      " 11776/115276 [==>...........................]       \n",
      " - ETA: 6s - loss: 5501.7165 - mae: 0.5920 - mean_absolute_percentage_error: 23420098.0000\n",
      "                                                    \n",
      " 13568/115276 [==>...........................]       \n",
      " - ETA: 6s - loss: 4988.6025 - mae: 0.5602 - mean_absolute_percentage_error: 23351282.0000\n",
      "                                                    \n",
      " 15616/115276 [===>..........................]       \n",
      " - ETA: 5s - loss: 4505.9537 - mae: 0.5320 - mean_absolute_percentage_error: 23568606.0000\n",
      "                                                    \n",
      " 17920/115276 [===>..........................]       \n",
      " - ETA: 4s - loss: 4063.0256 - mae: 0.5061 - mean_absolute_percentage_error: 23572968.0000\n",
      "                                                    \n",
      " 20480/115276 [====>.........................]       \n",
      " - ETA: 4s - loss: 3669.0259 - mae: 0.4821 - mean_absolute_percentage_error: 23859812.0000\n",
      "                                                    \n",
      " 22528/115276 [====>.........................]       \n",
      " - ETA: 4s - loss: 3408.1166 - mae: 0.4663 - mean_absolute_percentage_error: 24437210.0000\n",
      "                                                    \n",
      " 24832/115276 [=====>........................]       \n",
      " - ETA: 3s - loss: 3157.8097 - mae: 0.4507 - mean_absolute_percentage_error: 24812924.0000\n",
      "                                                    \n",
      " 26880/115276 [=====>........................]       \n",
      " - ETA: 3s - loss: 2965.7348 - mae: 0.4391 - mean_absolute_percentage_error: 25632408.0000\n",
      "                                                    \n",
      " 28928/115276 [======>.......................]       \n",
      " - ETA: 3s - loss: 2798.0484 - mae: 0.4285 - mean_absolute_percentage_error: 26195676.0000\n",
      "                                                    \n",
      " 31232/115276 [=======>......................]       \n",
      " - ETA: 3s - loss: 2632.1446 - mae: 0.4183 - mean_absolute_percentage_error: 26721116.0000\n",
      "                                                    \n",
      " 33536/115276 [=======>......................]       \n",
      " - ETA: 3s - loss: 2487.2015 - mae: 0.4086 - mean_absolute_percentage_error: 26965632.0000\n",
      "                                                    \n",
      " 36096/115276 [========>.....................]       \n",
      " - ETA: 2s - loss: 2345.0734 - mae: 0.3995 - mean_absolute_percentage_error: 27315452.0000\n",
      "                                                    \n",
      " 38656/115276 [=========>....................]       \n",
      " - ETA: 2s - loss: 2221.4555 - mae: 0.3919 - mean_absolute_percentage_error: 27636194.0000\n",
      "                                                    \n",
      " 40448/115276 [=========>....................]       \n",
      " - ETA: 2s - loss: 2143.2229 - mae: 0.3868 - mean_absolute_percentage_error: 27893442.0000\n",
      "                                                    \n",
      " 41728/115276 [=========>....................]       \n",
      " - ETA: 2s - loss: 2090.7830 - mae: 0.3835 - mean_absolute_percentage_error: 28007524.0000\n",
      "                                                    \n",
      " 43008/115276 [==========>...................]       \n",
      " - ETA: 2s - loss: 2041.2081 - mae: 0.3805 - mean_absolute_percentage_error: 28219516.0000\n",
      "                                                    \n",
      " 44288/115276 [==========>...................]       \n",
      " - ETA: 2s - loss: 1993.6946 - mae: 0.3778 - mean_absolute_percentage_error: 28316840.0000\n",
      "                                                    \n",
      " 45824/115276 [==========>...................]       \n",
      " - ETA: 2s - loss: 1940.1466 - mae: 0.3747 - mean_absolute_percentage_error: 28408946.0000\n",
      "                                                    \n",
      " 47616/115276 [===========>..................]       \n",
      " - ETA: 2s - loss: 1881.5348 - mae: 0.3708 - mean_absolute_percentage_error: 28452754.0000\n",
      "                                                    \n",
      " 49920/115276 [===========>..................]       \n",
      " - ETA: 2s - loss: 1811.8871 - mae: 0.3667 - mean_absolute_percentage_error: 28541462.0000\n",
      "                                                    \n",
      " 52224/115276 [============>.................]       \n",
      " - ETA: 2s - loss: 1747.6224 - mae: 0.3631 - mean_absolute_percentage_error: 28813084.0000\n",
      "                                                    \n",
      " 54784/115276 [=============>................]       \n",
      " - ETA: 2s - loss: 1682.2097 - mae: 0.3594 - mean_absolute_percentage_error: 29006140.0000\n",
      "                                                    \n",
      " 57088/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 1627.7551 - mae: 0.3563 - mean_absolute_percentage_error: 29147538.0000\n",
      "                                                    \n",
      " 59392/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 1576.8119 - mae: 0.3534 - mean_absolute_percentage_error: 29245158.0000\n",
      "                                                    \n",
      " 61696/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 1529.9022 - mae: 0.3507 - mean_absolute_percentage_error: 29489278.0000\n",
      "                                                    \n",
      " 64256/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 1481.4482 - mae: 0.3480 - mean_absolute_percentage_error: 29628740.0000\n",
      "                                                    \n",
      " 66560/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 1440.9943 - mae: 0.3457 - mean_absolute_percentage_error: 29666516.0000\n",
      "                                                    \n",
      " 68608/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 1407.2196 - mae: 0.3437 - mean_absolute_percentage_error: 29784098.0000\n",
      "                                                    \n",
      " 70912/115276 [=================>............]       \n",
      " - ETA: 1s - loss: 1371.4319 - mae: 0.3419 - mean_absolute_percentage_error: 29826012.0000\n",
      "                                                    \n",
      " 72960/115276 [=================>............]       \n",
      " - ETA: 1s - loss: 1341.5733 - mae: 0.3401 - mean_absolute_percentage_error: 29863896.0000\n",
      "                                                    \n",
      " 75520/115276 [==================>...........]       \n",
      " - ETA: 1s - loss: 1306.2286 - mae: 0.3383 - mean_absolute_percentage_error: 30047632.0000\n",
      "                                                    \n",
      " 77824/115276 [===================>..........]       \n",
      " - ETA: 1s - loss: 1276.4745 - mae: 0.3366 - mean_absolute_percentage_error: 30173424.0000\n",
      "                                                    \n",
      " 80128/115276 [===================>..........]       \n",
      " - ETA: 1s - loss: 1248.3042 - mae: 0.3352 - mean_absolute_percentage_error: 30174644.0000\n",
      "                                                    \n",
      " 82432/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 1221.9818 - mae: 0.3336 - mean_absolute_percentage_error: 30311948.0000\n",
      "                                                    \n",
      " 84736/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 1197.3086 - mae: 0.3321 - mean_absolute_percentage_error: 30388512.0000\n",
      "                                                    \n",
      " 87040/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 1173.7702 - mae: 0.3305 - mean_absolute_percentage_error: 30457016.0000\n",
      "                                                    \n",
      " 89088/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 1154.0470 - mae: 0.3293 - mean_absolute_percentage_error: 30406348.0000\n",
      "                                                    \n",
      " 91648/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 1130.4655 - mae: 0.3280 - mean_absolute_percentage_error: 30527456.0000\n",
      "                                                    \n",
      " 93952/115276 [=======================>......]       \n",
      " - ETA: 0s - loss: 1110.1813 - mae: 0.3268 - mean_absolute_percentage_error: 30537220.0000\n",
      "                                                    \n",
      " 96256/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 1091.2609 - mae: 0.3256 - mean_absolute_percentage_error: 30566806.0000\n",
      "                                                    \n",
      " 98560/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 1073.3512 - mae: 0.3244 - mean_absolute_percentage_error: 30467072.0000\n",
      "                                                    \n",
      "101120/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 1054.2158 - mae: 0.3233 - mean_absolute_percentage_error: 30425698.0000\n",
      "                                                    \n",
      "103680/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 1036.0549 - mae: 0.3223 - mean_absolute_percentage_error: 30510484.0000\n",
      "                                                    \n",
      "106240/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 1019.1444 - mae: 0.3214 - mean_absolute_percentage_error: 30551496.0000\n",
      "                                                    \n",
      "108544/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 1004.7792 - mae: 0.3207 - mean_absolute_percentage_error: 30668344.0000\n",
      "                                                    \n",
      "111104/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 989.4594 - mae: 0.3197 - mean_absolute_percentage_error: 30686252.0000 \n",
      "                                                     \n",
      "113408/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 976.4305 - mae: 0.3189 - mean_absolute_percentage_error: 30702234.0000\n",
      "                                                     \n",
      "115276/115276 [==============================]       \n",
      " - 3s 29us/step - loss: 966.1808 - mae: 0.3182 - mean_absolute_percentage_error: 30723040.0000 - val_loss: 371.1769 - val_mae: 0.2994 - val_mean_absolute_percentage_error: 14537079.0000\n",
      "\n",
      "Epoch 2/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 3s - loss: 519.2388 - mae: 0.3064 - mean_absolute_percentage_error: 39618392.0000\n",
      "                                                     \n",
      "  2816/115276 [..............................]       \n",
      " - ETA: 2s - loss: 420.0661 - mae: 0.2828 - mean_absolute_percentage_error: 34407836.0000\n",
      "                                                     \n",
      "  5120/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 405.7849 - mae: 0.2816 - mean_absolute_percentage_error: 32985302.0000\n",
      "                                                     \n",
      "  7424/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 392.2177 - mae: 0.2824 - mean_absolute_percentage_error: 32797772.0000\n",
      "                                                     \n",
      "  9984/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 382.8756 - mae: 0.2803 - mean_absolute_percentage_error: 32068500.0000\n",
      "                                                     \n",
      " 13056/115276 [==>...........................]       \n",
      " - ETA: 2s - loss: 372.7510 - mae: 0.2805 - mean_absolute_percentage_error: 32775582.0000\n",
      "                                                     \n",
      " 15872/115276 [===>..........................]       \n",
      " - ETA: 1s - loss: 371.7034 - mae: 0.2807 - mean_absolute_percentage_error: 32712960.0000\n",
      "                                                     \n",
      " 18176/115276 [===>..........................]       \n",
      " - ETA: 1s - loss: 369.3297 - mae: 0.2795 - mean_absolute_percentage_error: 32073732.0000\n",
      "                                                     \n",
      " 20480/115276 [====>.........................]       \n",
      " - ETA: 1s - loss: 367.7606 - mae: 0.2793 - mean_absolute_percentage_error: 32248422.0000\n",
      "                                                     \n",
      " 22528/115276 [====>.........................]       \n",
      " - ETA: 1s - loss: 365.5437 - mae: 0.2793 - mean_absolute_percentage_error: 32411494.0000\n",
      "                                                     \n",
      " 24576/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 364.2317 - mae: 0.2785 - mean_absolute_percentage_error: 32233242.0000\n",
      "                                                     \n",
      " 26624/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 361.8149 - mae: 0.2792 - mean_absolute_percentage_error: 32305874.0000\n",
      "                                                     \n",
      " 28928/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 359.1871 - mae: 0.2797 - mean_absolute_percentage_error: 32376476.0000\n",
      "                                                     \n",
      " 30720/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 357.2446 - mae: 0.2801 - mean_absolute_percentage_error: 32515454.0000\n",
      "                                                     \n",
      " 33024/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 355.0043 - mae: 0.2794 - mean_absolute_percentage_error: 32351618.0000\n",
      "                                                     \n",
      " 35840/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 353.8155 - mae: 0.2797 - mean_absolute_percentage_error: 32380936.0000\n",
      "                                                     \n",
      " 38144/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 352.3188 - mae: 0.2796 - mean_absolute_percentage_error: 32348844.0000\n",
      "                                                     \n",
      " 40448/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 349.9668 - mae: 0.2800 - mean_absolute_percentage_error: 32502006.0000\n",
      "                                                     \n",
      " 42496/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 348.6926 - mae: 0.2799 - mean_absolute_percentage_error: 32457616.0000\n",
      "                                                     \n",
      " 44544/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 348.0561 - mae: 0.2799 - mean_absolute_percentage_error: 32501528.0000\n",
      "                                                     \n",
      " 46848/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 347.1333 - mae: 0.2797 - mean_absolute_percentage_error: 32258864.0000\n",
      "                                                     \n",
      " 49152/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 345.8626 - mae: 0.2800 - mean_absolute_percentage_error: 32216064.0000\n",
      "                                                     \n",
      " 51200/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 344.7068 - mae: 0.2797 - mean_absolute_percentage_error: 32122332.0000\n",
      "                                                     \n",
      " 53760/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 343.6958 - mae: 0.2795 - mean_absolute_percentage_error: 31920072.0000\n",
      "                                                     \n",
      " 56576/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 344.1509 - mae: 0.2799 - mean_absolute_percentage_error: 32018810.0000\n",
      "                                                     \n",
      " 59392/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 344.7478 - mae: 0.2800 - mean_absolute_percentage_error: 32132856.0000\n",
      "                                                     \n",
      " 62208/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 344.2903 - mae: 0.2796 - mean_absolute_percentage_error: 32094924.0000\n",
      "                                                     \n",
      " 65024/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 344.1128 - mae: 0.2799 - mean_absolute_percentage_error: 32119772.0000\n",
      "                                                     \n",
      " 67840/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 344.1917 - mae: 0.2799 - mean_absolute_percentage_error: 32153532.0000\n",
      "                                                     \n",
      " 70400/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 344.2305 - mae: 0.2800 - mean_absolute_percentage_error: 32220066.0000\n",
      "                                                     \n",
      " 73216/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 344.7744 - mae: 0.2800 - mean_absolute_percentage_error: 32171922.0000\n",
      "                                                     \n",
      " 76032/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 345.1752 - mae: 0.2798 - mean_absolute_percentage_error: 32211958.0000\n",
      "                                                     \n",
      " 78592/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 345.9357 - mae: 0.2798 - mean_absolute_percentage_error: 32295182.0000\n",
      "                                                     \n",
      " 81408/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 346.5812 - mae: 0.2797 - mean_absolute_percentage_error: 32285574.0000\n",
      "                                                     \n",
      " 84224/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 346.6445 - mae: 0.2799 - mean_absolute_percentage_error: 32324990.0000\n",
      "                                                     \n",
      " 86784/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 346.6361 - mae: 0.2800 - mean_absolute_percentage_error: 32296044.0000\n",
      "                                                     \n",
      " 89088/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 346.6646 - mae: 0.2802 - mean_absolute_percentage_error: 32309718.0000\n",
      "                                                     \n",
      " 91136/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 347.2457 - mae: 0.2802 - mean_absolute_percentage_error: 32324412.0000\n",
      "                                                     \n",
      " 93696/115276 [=======================>......]       \n",
      " - ETA: 0s - loss: 348.5445 - mae: 0.2804 - mean_absolute_percentage_error: 32310104.0000\n",
      "                                                     \n",
      " 96256/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 349.7086 - mae: 0.2804 - mean_absolute_percentage_error: 32282896.0000\n",
      "                                                     \n",
      " 98816/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 350.6865 - mae: 0.2805 - mean_absolute_percentage_error: 32315614.0000\n",
      "                                                     \n",
      "100864/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 351.3239 - mae: 0.2805 - mean_absolute_percentage_error: 32269162.0000\n",
      "                                                     \n",
      "102400/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 351.8597 - mae: 0.2805 - mean_absolute_percentage_error: 32245608.0000\n",
      "                                                     \n",
      "104960/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 352.5977 - mae: 0.2806 - mean_absolute_percentage_error: 32300968.0000\n",
      "                                                     \n",
      "107520/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 353.4444 - mae: 0.2807 - mean_absolute_percentage_error: 32297218.0000\n",
      "                                                     \n",
      "109824/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 354.1572 - mae: 0.2807 - mean_absolute_percentage_error: 32305682.0000\n",
      "                                                     \n",
      "112384/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 354.7050 - mae: 0.2807 - mean_absolute_percentage_error: 32287632.0000\n",
      "                                                     \n",
      "114688/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 355.0698 - mae: 0.2806 - mean_absolute_percentage_error: 32259060.0000\n",
      "                                                     \n",
      "115276/115276 [==============================]       \n",
      " - 2s 21us/step - loss: 355.1272 - mae: 0.2806 - mean_absolute_percentage_error: 32217604.0000 - val_loss: 404.8699 - val_mae: 0.3001 - val_mean_absolute_percentage_error: 14538465.0000\n",
      "\n",
      "Epoch 3/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 4s - loss: 573.5050 - mae: 0.2699 - mean_absolute_percentage_error: 29114436.0000\n",
      "                                                     \n",
      "  2304/115276 [..............................]       \n",
      " - ETA: 3s - loss: 466.0533 - mae: 0.2792 - mean_absolute_percentage_error: 32137578.0000\n",
      "                                                     \n",
      "  4864/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 448.7464 - mae: 0.2813 - mean_absolute_percentage_error: 34336704.0000\n",
      "                                                     \n",
      "  7424/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 439.9473 - mae: 0.2799 - mean_absolute_percentage_error: 33158376.0000\n",
      "                                                     \n",
      "  9728/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 437.2935 - mae: 0.2798 - mean_absolute_percentage_error: 33046184.0000\n",
      "                                                     \n",
      " 12288/115276 [==>...........................]       \n",
      " - ETA: 2s - loss: 432.2077 - mae: 0.2781 - mean_absolute_percentage_error: 32660634.0000\n",
      "                                                     \n",
      " 14848/115276 [==>...........................]       \n",
      " - ETA: 2s - loss: 426.3491 - mae: 0.2776 - mean_absolute_percentage_error: 32381308.0000\n",
      "                                                     \n",
      " 17408/115276 [===>..........................]       \n",
      " - ETA: 2s - loss: 420.9962 - mae: 0.2772 - mean_absolute_percentage_error: 31662324.0000\n",
      "                                                     \n",
      " 20224/115276 [====>.........................]       \n",
      " - ETA: 2s - loss: 414.4657 - mae: 0.2781 - mean_absolute_percentage_error: 32173546.0000\n",
      "                                                     \n",
      " 22784/115276 [====>.........................]       \n",
      " - ETA: 1s - loss: 411.8693 - mae: 0.2779 - mean_absolute_percentage_error: 32243438.0000\n",
      "                                                     \n",
      " 25344/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 410.2071 - mae: 0.2782 - mean_absolute_percentage_error: 32120374.0000\n",
      "                                                     \n",
      " 27648/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 405.5395 - mae: 0.2788 - mean_absolute_percentage_error: 32596698.0000\n",
      "                                                     \n",
      " 30208/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 402.3479 - mae: 0.2793 - mean_absolute_percentage_error: 32709560.0000\n",
      "                                                     \n",
      " 33024/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 398.5339 - mae: 0.2796 - mean_absolute_percentage_error: 32487040.0000\n",
      "                                                     \n",
      " 35840/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 397.4963 - mae: 0.2797 - mean_absolute_percentage_error: 32489760.0000\n",
      "                                                     \n",
      " 38912/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 396.4251 - mae: 0.2796 - mean_absolute_percentage_error: 32201340.0000\n",
      "                                                     \n",
      " 41984/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 394.2239 - mae: 0.2794 - mean_absolute_percentage_error: 32150556.0000\n",
      "                                                     \n",
      " 44800/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 391.8977 - mae: 0.2792 - mean_absolute_percentage_error: 32109408.0000\n",
      "                                                     \n",
      " 47616/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 390.0845 - mae: 0.2789 - mean_absolute_percentage_error: 31942516.0000\n",
      "                                                     \n",
      " 50432/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 390.5813 - mae: 0.2789 - mean_absolute_percentage_error: 32058660.0000\n",
      "                                                     \n",
      " 53248/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 391.0178 - mae: 0.2793 - mean_absolute_percentage_error: 32106762.0000\n",
      "                                                     \n",
      " 56064/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 391.4798 - mae: 0.2795 - mean_absolute_percentage_error: 32161948.0000\n",
      "                                                     \n",
      " 58880/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 392.3780 - mae: 0.2799 - mean_absolute_percentage_error: 32173910.0000\n",
      "                                                     \n",
      " 61952/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 392.5658 - mae: 0.2801 - mean_absolute_percentage_error: 32488450.0000\n",
      "                                                     \n",
      " 65024/115276 [===============>..............]       \n",
      " - ETA: 0s - loss: 392.4258 - mae: 0.2803 - mean_absolute_percentage_error: 32445202.0000\n",
      "                                                     \n",
      " 68096/115276 [================>.............]       \n",
      " - ETA: 0s - loss: 392.4884 - mae: 0.2804 - mean_absolute_percentage_error: 32470104.0000\n",
      "                                                     \n",
      " 70656/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 392.4234 - mae: 0.2804 - mean_absolute_percentage_error: 32502680.0000\n",
      "                                                     \n",
      " 73216/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 392.6760 - mae: 0.2805 - mean_absolute_percentage_error: 32469182.0000\n",
      "                                                     \n",
      " 75776/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 392.4517 - mae: 0.2805 - mean_absolute_percentage_error: 32490694.0000\n",
      "                                                     \n",
      " 78080/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 392.6780 - mae: 0.2805 - mean_absolute_percentage_error: 32357736.0000\n",
      "                                                     \n",
      " 80384/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 393.8737 - mae: 0.2802 - mean_absolute_percentage_error: 32236822.0000\n",
      "                                                     \n",
      " 82688/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 394.6581 - mae: 0.2804 - mean_absolute_percentage_error: 32247310.0000\n",
      "                                                     \n",
      " 84992/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 394.8877 - mae: 0.2805 - mean_absolute_percentage_error: 32251248.0000\n",
      "                                                     \n",
      " 87040/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 395.5835 - mae: 0.2805 - mean_absolute_percentage_error: 32253570.0000\n",
      "                                                     \n",
      " 89344/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 396.2888 - mae: 0.2804 - mean_absolute_percentage_error: 32137166.0000\n",
      "                                                     \n",
      " 91904/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 397.4250 - mae: 0.2803 - mean_absolute_percentage_error: 32047512.0000\n",
      "                                                     \n",
      " 94976/115276 [=======================>......]       \n",
      " - ETA: 0s - loss: 398.5082 - mae: 0.2804 - mean_absolute_percentage_error: 32139992.0000\n",
      "                                                     \n",
      " 98048/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 399.3319 - mae: 0.2804 - mean_absolute_percentage_error: 32096384.0000\n",
      "                                                     \n",
      "100864/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 399.9931 - mae: 0.2804 - mean_absolute_percentage_error: 32082460.0000\n",
      "                                                     \n",
      "103424/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 399.9228 - mae: 0.2804 - mean_absolute_percentage_error: 32092834.0000\n",
      "                                                     \n",
      "105984/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 399.4902 - mae: 0.2804 - mean_absolute_percentage_error: 32126196.0000\n",
      "                                                     \n",
      "108544/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 399.9448 - mae: 0.2804 - mean_absolute_percentage_error: 32182486.0000\n",
      "                                                     \n",
      "111104/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 400.3788 - mae: 0.2804 - mean_absolute_percentage_error: 32147126.0000\n",
      "                                                     \n",
      "113664/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 400.3289 - mae: 0.2804 - mean_absolute_percentage_error: 32156162.0000\n",
      "                                                     \n",
      "115276/115276 [==============================]       \n",
      " - 2s 20us/step - loss: 399.9663 - mae: 0.2805 - mean_absolute_percentage_error: 32219768.0000 - val_loss: 384.4962 - val_mae: 0.3005 - val_mean_absolute_percentage_error: 14552259.0000\n",
      "\n",
      "Epoch 4/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 4s - loss: 536.9322 - mae: 0.2677 - mean_absolute_percentage_error: 26598540.0000\n",
      "                                                     \n",
      "  2816/115276 [..............................]       \n",
      " - ETA: 2s - loss: 434.2685 - mae: 0.2671 - mean_absolute_percentage_error: 29105186.0000\n",
      "                                                     \n",
      "  4352/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 430.6174 - mae: 0.2726 - mean_absolute_percentage_error: 29631312.0000\n",
      "                                                     \n",
      "  5888/115276 [>.............................]       \n",
      " - ETA: 3s - loss: 428.8034 - mae: 0.2741 - mean_absolute_percentage_error: 30397018.0000\n",
      "                                                     \n",
      "  8192/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 421.5002 - mae: 0.2766 - mean_absolute_percentage_error: 30289340.0000\n",
      "                                                     \n",
      " 10752/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 418.4569 - mae: 0.2766 - mean_absolute_percentage_error: 30746858.0000\n",
      "                                                     \n",
      " 13056/115276 [==>...........................]       \n",
      " - ETA: 2s - loss: 413.9257 - mae: 0.2765 - mean_absolute_percentage_error: 30363258.0000\n",
      "                                                     \n",
      " 15616/115276 [===>..........................]       \n",
      " - ETA: 2s - loss: 411.4492 - mae: 0.2781 - mean_absolute_percentage_error: 31395404.0000\n",
      "                                                     \n",
      " 17920/115276 [===>..........................]       \n",
      " - ETA: 2s - loss: 412.7783 - mae: 0.2771 - mean_absolute_percentage_error: 31546094.0000\n",
      "                                                     \n",
      " 20736/115276 [====>.........................]       \n",
      " - ETA: 2s - loss: 412.4418 - mae: 0.2774 - mean_absolute_percentage_error: 31664368.0000\n",
      "                                                     \n",
      " 23552/115276 [=====>........................]       \n",
      " - ETA: 2s - loss: 411.1896 - mae: 0.2784 - mean_absolute_percentage_error: 31610882.0000\n",
      "                                                     \n",
      " 26624/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 413.5853 - mae: 0.2788 - mean_absolute_percentage_error: 31426390.0000\n",
      "                                                     \n",
      " 29696/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 416.3977 - mae: 0.2794 - mean_absolute_percentage_error: 31619536.0000\n",
      "                                                     \n",
      " 32512/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 416.9036 - mae: 0.2793 - mean_absolute_percentage_error: 31697242.0000\n",
      "                                                     \n",
      " 35328/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 417.4625 - mae: 0.2796 - mean_absolute_percentage_error: 31713280.0000\n",
      "                                                     \n",
      " 37376/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 417.9319 - mae: 0.2795 - mean_absolute_percentage_error: 31857482.0000\n",
      "                                                     \n",
      " 39680/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 417.8919 - mae: 0.2797 - mean_absolute_percentage_error: 32159496.0000\n",
      "                                                     \n",
      " 42496/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 418.9369 - mae: 0.2797 - mean_absolute_percentage_error: 32063466.0000\n",
      "                                                     \n",
      " 45056/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 419.8218 - mae: 0.2798 - mean_absolute_percentage_error: 32059576.0000\n",
      "                                                     \n",
      " 47360/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 419.2043 - mae: 0.2798 - mean_absolute_percentage_error: 31928570.0000\n",
      "                                                     \n",
      " 49664/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 418.7636 - mae: 0.2798 - mean_absolute_percentage_error: 31809980.0000\n",
      "                                                     \n",
      " 52224/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 419.1654 - mae: 0.2798 - mean_absolute_percentage_error: 31748502.0000\n",
      "                                                     \n",
      " 54272/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 419.4507 - mae: 0.2801 - mean_absolute_percentage_error: 31889290.0000\n",
      "                                                     \n",
      " 56320/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 419.5313 - mae: 0.2802 - mean_absolute_percentage_error: 31884282.0000\n",
      "                                                     \n",
      " 58624/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 419.8921 - mae: 0.2800 - mean_absolute_percentage_error: 31972156.0000\n",
      "                                                     \n",
      " 60928/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 420.2639 - mae: 0.2802 - mean_absolute_percentage_error: 32019510.0000\n",
      "                                                     \n",
      " 63232/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 421.6038 - mae: 0.2803 - mean_absolute_percentage_error: 31966388.0000\n",
      "                                                     \n",
      " 65280/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 423.3819 - mae: 0.2802 - mean_absolute_percentage_error: 31975344.0000\n",
      "                                                     \n",
      " 67584/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 424.2684 - mae: 0.2803 - mean_absolute_percentage_error: 32002330.0000\n",
      "                                                     \n",
      " 70144/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 424.5070 - mae: 0.2803 - mean_absolute_percentage_error: 32067954.0000\n",
      "                                                     \n",
      " 72704/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 424.0178 - mae: 0.2805 - mean_absolute_percentage_error: 32170560.0000\n",
      "                                                     \n",
      " 75008/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 423.6865 - mae: 0.2804 - mean_absolute_percentage_error: 32077642.0000\n",
      "                                                     \n",
      " 77568/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 423.1884 - mae: 0.2803 - mean_absolute_percentage_error: 32148770.0000\n",
      "                                                     \n",
      " 79616/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 422.3602 - mae: 0.2805 - mean_absolute_percentage_error: 32154670.0000\n",
      "                                                     \n",
      " 81920/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 421.5508 - mae: 0.2805 - mean_absolute_percentage_error: 32109548.0000\n",
      "                                                     \n",
      " 84224/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 421.2101 - mae: 0.2806 - mean_absolute_percentage_error: 32171198.0000\n",
      "                                                     \n",
      " 86528/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 420.6585 - mae: 0.2808 - mean_absolute_percentage_error: 32142318.0000\n",
      "                                                     \n",
      " 89088/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 419.9692 - mae: 0.2809 - mean_absolute_percentage_error: 32226914.0000\n",
      "                                                     \n",
      " 91648/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 419.1561 - mae: 0.2805 - mean_absolute_percentage_error: 32152524.0000\n",
      "                                                     \n",
      " 93952/115276 [=======================>......]       \n",
      " - ETA: 0s - loss: 418.2922 - mae: 0.2804 - mean_absolute_percentage_error: 32211890.0000\n",
      "                                                     \n",
      " 96512/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 417.7693 - mae: 0.2805 - mean_absolute_percentage_error: 32327430.0000\n",
      "                                                     \n",
      " 99328/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 417.6149 - mae: 0.2804 - mean_absolute_percentage_error: 32253546.0000\n",
      "                                                     \n",
      "102144/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 418.1427 - mae: 0.2801 - mean_absolute_percentage_error: 32141730.0000\n",
      "                                                     \n",
      "104704/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 418.6189 - mae: 0.2802 - mean_absolute_percentage_error: 32096008.0000\n",
      "                                                     \n",
      "107008/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 419.2075 - mae: 0.2802 - mean_absolute_percentage_error: 32166726.0000\n",
      "                                                     \n",
      "109312/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 419.2551 - mae: 0.2804 - mean_absolute_percentage_error: 32213502.0000\n",
      "                                                     \n",
      "111616/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 419.7785 - mae: 0.2804 - mean_absolute_percentage_error: 32222662.0000\n",
      "                                                     \n",
      "114176/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 420.4877 - mae: 0.2806 - mean_absolute_percentage_error: 32246930.0000\n",
      "                                                     \n",
      "115276/115276 [==============================]       \n",
      " - 2s 22us/step - loss: 420.5852 - mae: 0.2805 - mean_absolute_percentage_error: 32214228.0000 - val_loss: 434.4720 - val_mae: 0.2981 - val_mean_absolute_percentage_error: 14535985.0000\n",
      "\n",
      "Epoch 5/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 4s - loss: 609.2956 - mae: 0.2732 - mean_absolute_percentage_error: 28655546.0000\n",
      "                                                     \n",
      "  2560/115276 [..............................]       \n",
      " - ETA: 2s - loss: 482.7040 - mae: 0.2791 - mean_absolute_percentage_error: 33679980.0000\n",
      "                                                     \n",
      "  4608/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 466.7095 - mae: 0.2817 - mean_absolute_percentage_error: 34071604.0000\n",
      "                                                     \n",
      "  6656/115276 [>.............................]       \n",
      " - ETA: 2s - loss: 459.3297 - mae: 0.2812 - mean_absolute_percentage_error: 34003624.0000\n",
      "                                                     \n",
      "  8704/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 457.8717 - mae: 0.2812 - mean_absolute_percentage_error: 33098710.0000\n",
      "                                                     \n",
      " 11008/115276 [=>............................]       \n",
      " - ETA: 2s - loss: 449.7997 - mae: 0.2814 - mean_absolute_percentage_error: 32702834.0000\n",
      "                                                     \n",
      " 13312/115276 [==>...........................]       \n",
      " - ETA: 2s - loss: 444.6325 - mae: 0.2820 - mean_absolute_percentage_error: 32884318.0000\n",
      "                                                     \n",
      " 15872/115276 [===>..........................]       \n",
      " - ETA: 2s - loss: 439.6290 - mae: 0.2809 - mean_absolute_percentage_error: 32734356.0000\n",
      "                                                     \n",
      " 17408/115276 [===>..........................]       \n",
      " - ETA: 2s - loss: 437.2208 - mae: 0.2811 - mean_absolute_percentage_error: 32798958.0000\n",
      "                                                     \n",
      " 19712/115276 [====>.........................]       \n",
      " - ETA: 2s - loss: 436.5742 - mae: 0.2805 - mean_absolute_percentage_error: 32657498.0000\n",
      "                                                     \n",
      " 22016/115276 [====>.........................]       \n",
      " - ETA: 2s - loss: 438.1300 - mae: 0.2811 - mean_absolute_percentage_error: 32498510.0000\n",
      "                                                     \n",
      " 24320/115276 [=====>........................]       \n",
      " - ETA: 2s - loss: 438.1825 - mae: 0.2811 - mean_absolute_percentage_error: 32229300.0000\n",
      "                                                     \n",
      " 26624/115276 [=====>........................]       \n",
      " - ETA: 2s - loss: 437.9091 - mae: 0.2811 - mean_absolute_percentage_error: 32171010.0000\n",
      "                                                     \n",
      " 29184/115276 [======>.......................]       \n",
      " - ETA: 2s - loss: 438.5038 - mae: 0.2804 - mean_absolute_percentage_error: 32016560.0000\n",
      "                                                     \n",
      " 31488/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 437.8177 - mae: 0.2807 - mean_absolute_percentage_error: 32049666.0000\n",
      "                                                     \n",
      " 33536/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 437.7817 - mae: 0.2805 - mean_absolute_percentage_error: 31924988.0000\n",
      "                                                     \n",
      " 35328/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 437.9603 - mae: 0.2805 - mean_absolute_percentage_error: 31864164.0000\n",
      "                                                     \n",
      " 37376/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 439.4052 - mae: 0.2803 - mean_absolute_percentage_error: 31777658.0000\n",
      "                                                     \n",
      " 39680/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 440.7631 - mae: 0.2805 - mean_absolute_percentage_error: 31838816.0000\n",
      "                                                     \n",
      " 41984/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 441.4553 - mae: 0.2807 - mean_absolute_percentage_error: 31804066.0000\n",
      "                                                     \n",
      " 44800/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 441.2346 - mae: 0.2805 - mean_absolute_percentage_error: 31876916.0000\n",
      "                                                     \n",
      " 47616/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 440.1449 - mae: 0.2805 - mean_absolute_percentage_error: 31860932.0000\n",
      "                                                     \n",
      " 50176/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 438.5278 - mae: 0.2803 - mean_absolute_percentage_error: 31846368.0000\n",
      "                                                     \n",
      " 52480/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 437.7881 - mae: 0.2801 - mean_absolute_percentage_error: 31875712.0000\n",
      "                                                     \n",
      " 54784/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 436.8811 - mae: 0.2797 - mean_absolute_percentage_error: 31952184.0000\n",
      "                                                     \n",
      " 57088/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 437.8800 - mae: 0.2797 - mean_absolute_percentage_error: 31901294.0000\n",
      "                                                     \n",
      " 59392/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 439.0195 - mae: 0.2799 - mean_absolute_percentage_error: 31934714.0000\n",
      "                                                     \n",
      " 61696/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 440.4387 - mae: 0.2802 - mean_absolute_percentage_error: 32044682.0000\n",
      "                                                     \n",
      " 64000/115276 [===============>..............]       \n",
      " - ETA: 1s - loss: 441.4923 - mae: 0.2799 - mean_absolute_percentage_error: 32051484.0000\n",
      "                                                     \n",
      " 65792/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 442.3877 - mae: 0.2802 - mean_absolute_percentage_error: 32162918.0000\n",
      "                                                     \n",
      " 67584/115276 [================>.............]       \n",
      " - ETA: 1s - loss: 442.7020 - mae: 0.2803 - mean_absolute_percentage_error: 32129680.0000\n",
      "                                                     \n",
      " 69888/115276 [=================>............]       \n",
      " - ETA: 1s - loss: 442.3984 - mae: 0.2804 - mean_absolute_percentage_error: 32276754.0000\n",
      "                                                     \n",
      " 72192/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 441.8071 - mae: 0.2803 - mean_absolute_percentage_error: 32341580.0000\n",
      "                                                     \n",
      " 74752/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 441.0338 - mae: 0.2803 - mean_absolute_percentage_error: 32169858.0000\n",
      "                                                     \n",
      " 77056/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 440.6205 - mae: 0.2802 - mean_absolute_percentage_error: 32116266.0000\n",
      "                                                     \n",
      " 79616/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 440.1170 - mae: 0.2804 - mean_absolute_percentage_error: 32188030.0000\n",
      "                                                     \n",
      " 81664/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 440.0241 - mae: 0.2802 - mean_absolute_percentage_error: 32147964.0000\n",
      "                                                     \n",
      " 83968/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 439.6985 - mae: 0.2800 - mean_absolute_percentage_error: 32165872.0000\n",
      "                                                     \n",
      " 86528/115276 [=====================>........]       \n",
      " - ETA: 0s - loss: 439.7869 - mae: 0.2800 - mean_absolute_percentage_error: 32169600.0000\n",
      "                                                     \n",
      " 89088/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 439.4168 - mae: 0.2802 - mean_absolute_percentage_error: 32285426.0000\n",
      "                                                     \n",
      " 91904/115276 [======================>.......]       \n",
      " - ETA: 0s - loss: 439.0383 - mae: 0.2802 - mean_absolute_percentage_error: 32305384.0000\n",
      "                                                     \n",
      " 94464/115276 [=======================>......]       \n",
      " - ETA: 0s - loss: 439.0999 - mae: 0.2805 - mean_absolute_percentage_error: 32308866.0000\n",
      "                                                     \n",
      " 97024/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 439.7036 - mae: 0.2805 - mean_absolute_percentage_error: 32314166.0000\n",
      "                                                     \n",
      " 99584/115276 [========================>.....]       \n",
      " - ETA: 0s - loss: 439.2761 - mae: 0.2804 - mean_absolute_percentage_error: 32295938.0000\n",
      "                                                     \n",
      "101632/115276 [=========================>....]       \n",
      " - ETA: 0s - loss: 438.7959 - mae: 0.2805 - mean_absolute_percentage_error: 32256684.0000\n",
      "                                                     \n",
      "104192/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 438.2456 - mae: 0.2804 - mean_absolute_percentage_error: 32214768.0000\n",
      "                                                     \n",
      "106752/115276 [==========================>...]       \n",
      " - ETA: 0s - loss: 437.7319 - mae: 0.2807 - mean_absolute_percentage_error: 32259232.0000\n",
      "                                                     \n",
      "109568/115276 [===========================>..]       \n",
      " - ETA: 0s - loss: 437.0210 - mae: 0.2808 - mean_absolute_percentage_error: 32298934.0000\n",
      "                                                     \n",
      "112384/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 436.4427 - mae: 0.2806 - mean_absolute_percentage_error: 32292500.0000\n",
      "                                                     \n",
      "114944/115276 [============================>.]       \n",
      " - ETA: 0s - loss: 435.8424 - mae: 0.2805 - mean_absolute_percentage_error: 32238318.0000\n",
      "                                                     \n",
      "115276/115276 [==============================]       \n",
      " - 3s 22us/step - loss: 435.5873 - mae: 0.2805 - mean_absolute_percentage_error: 32215298.0000 - val_loss: 404.6856 - val_mae: 0.2987 - val_mean_absolute_percentage_error: 14508261.0000\n",
      "\n",
      "Epoch 6/500                                          \n",
      "   256/115276 [..............................]       \n",
      " - ETA: 3s - loss: 551.3818 - mae: 0.3076 - mean_absolute_percentage_error: 43402596.0000\n",
      "                                                     \n",
      "  3072/115276 [..............................]       \n",
      " - ETA: 2s - loss: 446.5771 - mae: 0.2835 - mean_absolute_percentage_error: 31588512.0000\n",
      "                                                     \n",
      "  6144/115276 [>.............................]       \n",
      " - ETA: 1s - loss: 455.2730 - mae: 0.2855 - mean_absolute_percentage_error: 31367038.0000\n",
      "                                                     \n",
      "  9216/115276 [=>............................]       \n",
      " - ETA: 1s - loss: 450.3310 - mae: 0.2822 - mean_absolute_percentage_error: 31467666.0000\n",
      "                                                     \n",
      " 12032/115276 [==>...........................]       \n",
      " - ETA: 1s - loss: 446.4796 - mae: 0.2819 - mean_absolute_percentage_error: 31062962.0000\n",
      "                                                     \n",
      " 14592/115276 [==>...........................]       \n",
      " - ETA: 1s - loss: 444.7981 - mae: 0.2815 - mean_absolute_percentage_error: 30671268.0000\n",
      "                                                     \n",
      " 17664/115276 [===>..........................]       \n",
      " - ETA: 1s - loss: 447.2550 - mae: 0.2822 - mean_absolute_percentage_error: 31323136.0000\n",
      "                                                     \n",
      " 20736/115276 [====>.........................]       \n",
      " - ETA: 1s - loss: 447.3479 - mae: 0.2822 - mean_absolute_percentage_error: 31788048.0000\n",
      "                                                     \n",
      " 23808/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 447.1678 - mae: 0.2820 - mean_absolute_percentage_error: 32091364.0000\n",
      "                                                     \n",
      " 26368/115276 [=====>........................]       \n",
      " - ETA: 1s - loss: 447.2693 - mae: 0.2813 - mean_absolute_percentage_error: 31748612.0000\n",
      "                                                     \n",
      " 28672/115276 [======>.......................]       \n",
      " - ETA: 1s - loss: 447.4527 - mae: 0.2814 - mean_absolute_percentage_error: 31867916.0000\n",
      "                                                     \n",
      " 31488/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 447.8281 - mae: 0.2820 - mean_absolute_percentage_error: 32237182.0000\n",
      "                                                     \n",
      " 34560/115276 [=======>......................]       \n",
      " - ETA: 1s - loss: 446.9661 - mae: 0.2824 - mean_absolute_percentage_error: 32289808.0000\n",
      "                                                     \n",
      " 37632/115276 [========>.....................]       \n",
      " - ETA: 1s - loss: 445.6236 - mae: 0.2816 - mean_absolute_percentage_error: 32358366.0000\n",
      "                                                     \n",
      " 40448/115276 [=========>....................]       \n",
      " - ETA: 1s - loss: 444.2189 - mae: 0.2816 - mean_absolute_percentage_error: 32467116.0000\n",
      "                                                     \n",
      " 43264/115276 [==========>...................]       \n",
      " - ETA: 1s - loss: 443.3260 - mae: 0.2818 - mean_absolute_percentage_error: 32520576.0000\n",
      "                                                     \n",
      " 46592/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 441.8928 - mae: 0.2819 - mean_absolute_percentage_error: 32433988.0000\n",
      "                                                     \n",
      " 49408/115276 [===========>..................]       \n",
      " - ETA: 1s - loss: 441.7023 - mae: 0.2819 - mean_absolute_percentage_error: 32486628.0000\n",
      "                                                     \n",
      " 52224/115276 [============>.................]       \n",
      " - ETA: 1s - loss: 441.3119 - mae: 0.2816 - mean_absolute_percentage_error: 32334954.0000\n",
      "                                                     \n",
      " 55040/115276 [=============>................]       \n",
      " - ETA: 1s - loss: 441.1656 - mae: 0.2814 - mean_absolute_percentage_error: 32223354.0000\n",
      "                                                     \n",
      " 57856/115276 [==============>...............]       \n",
      " - ETA: 1s - loss: 440.6548 - mae: 0.2811 - mean_absolute_percentage_error: 32244286.0000\n",
      "                                                     \n",
      " 60672/115276 [==============>...............]       \n",
      " - ETA: 0s - loss: 440.1798 - mae: 0.2810 - mean_absolute_percentage_error: 32326160.0000\n",
      "                                                     \n",
      " 63488/115276 [===============>..............]       \n",
      " - ETA: 0s - loss: 440.2830 - mae: 0.2807 - mean_absolute_percentage_error: 32207604.0000\n",
      "                                                     \n",
      " 66048/115276 [================>.............]       \n",
      " - ETA: 0s - loss: 440.6498 - mae: 0.2808 - mean_absolute_percentage_error: 32286750.0000\n",
      "                                                     \n",
      " 68608/115276 [================>.............]       \n",
      " - ETA: 0s - loss: 440.8729 - mae: 0.2809 - mean_absolute_percentage_error: 32215182.0000\n",
      "                                                     \n",
      " 71168/115276 [=================>............]       \n",
      " - ETA: 0s - loss: 440.0635 - mae: 0.2808 - mean_absolute_percentage_error: 32133108.0000\n",
      "                                                     \n",
      " 73728/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 439.8212 - mae: 0.2812 - mean_absolute_percentage_error: 32175382.0000\n",
      "                                                     \n",
      " 76288/115276 [==================>...........]       \n",
      " - ETA: 0s - loss: 439.2116 - mae: 0.2812 - mean_absolute_percentage_error: 32150802.0000\n",
      "                                                     \n",
      " 78848/115276 [===================>..........]       \n",
      " - ETA: 0s - loss: 438.7837 - mae: 0.2811 - mean_absolute_percentage_error: 32055114.0000\n",
      "                                                     \n",
      " 81408/115276 [====================>.........]       \n",
      " - ETA: 0s - loss: 437.7301 - mae: 0.2812 - mean_absolute_percentage_error: 32142646.0000\n",
      "                                                     "
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='play_w_hyperas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:colorml] *",
   "language": "python",
   "name": "conda-env-colorml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
